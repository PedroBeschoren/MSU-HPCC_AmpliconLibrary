#!/bin/bash -login
 
#SBATCH --time=2:00:00					### limit of wall clock time - how long the job will run (same as -t)
#SBATCH --ntasks=1					### number of tasks - how many tasks (nodes) that you require (same as -n)
#SBATCH --cpus-per-task=20				### number of CPUs (or cores) per task (same as -c)
#SBATCH --mem=32G					### memory required per node - amount of memory (in bytes)
#SBATCH --job-name redeplicate				### you can give your job a name for easier identification (same as -J)

cd ${SLURM_SUBMIT_DIR}

module load GCC/5.4.0-2.26  OpenMPI/1.10.3-CUDA
module load seqtk
module load vsearch/2.9.1

mkdir dereplicated/

# dereplication 
/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -fastx_uniques filtered/filtered.fastq -fastqout dereplicated/uniques.fastq -sizeout

seqtk seq -aQ64 dereplicated/uniques.fastq >  dereplicated/uniques.fasta
seqtk sample -s400 dereplicated/uniques.fasta 500 > dereplicated/sub_uniques.fasta
/mnt/research/rdp/public/thirdParty/usearch11.0.667_i86linux64 -fastq_eestats2 dereplicated/uniques.fastq -output stats/eestats2_uniques.txt -length_cutoffs 100,500,1
vsearch -fastq_stats dereplicated/uniques.fastq -log stats/stats_results_uniques.txt

#NOTE: if you are about to merge unique sequences from different related libraries with cat such as in:
# cat Lib_5/dereplicated/uniques.fastq Lib_6/dereplicated/uniques.fastq Lib_7/dereplicated/uniques.fastq Lib_8/dereplicated/uniques.fastq > JointFungalLibrary/dereplicated/Cat_Lib5-8_uniques.fastq
# Don't forget to add the -sizein option for the next dereplication of dereplicated sequences
